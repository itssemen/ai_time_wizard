# Тестирование ML моделей

Этот скрипт предназначен для проверки качества работы обученных моделей из папки `ml/models_sklearn`.
Он оценивает:
1.  Модель извлечения именованных сущностей (NER) для задач (TASK).
2.  Модель регрессии для предсказания длительности выполнения задачи.
3.  Модель классификации для предсказания приоритета задачи.

## Зависимости

Все необходимые Python-библиотеки перечислены в файле `ml/requirements.txt`.
Для их установки выполните команду из корневой директории проекта:

```bash
pip install -r ml/requirements.txt
```

Также скрипт автоматически попытается загрузить необходимые ресурсы `nltk` (например, `punkt`), если они отсутствуют.

## Запуск скрипта тестирования

Для запуска скрипта проверки моделей выполните следующую команду из **корневой директории проекта**:

```bash
python ml/test_models/test_models.py
```

Или, если вы находитесь непосредственно в папке `ml/test_models`:
```bash
python test_models.py
```

## Описание работы скрипта

1.  **Загрузка данных**: Скрипт загружает датасет `ml/freeform_task_dataset.json`, который использовался для обучения. Оценка производится на всем этом датасете.
2.  **Загрузка моделей**: Загружаются все три обученные модели (`ner_model.pkl`, `duration_model.pkl`, `priority_model.pkl`) и соответствующий векторизатор для NER (`ner_vectorizer.pkl`) из папки `ml/models_sklearn/`.
3.  **Тестирование NER модели**:
    *   Для каждого текста в датасете предсказываются IOB-теги.
    *   Выводится отчет `classification_report` по качеству IOB-тегирования (сравниваются предсказанные теги с эталонными, сгенерированными на основе аннотаций в датасете).
    *   Для нескольких примеров из датасета выводятся: исходный текст, размеченные токены, эталонные IOB-теги, предсказанные IOB-теги, эталонные сущности и предсказанные сущности.
4.  **Тестирование модели длительности**:
    *   Для каждой задачи (извлеченной из эталонных аннотаций датасета) предсказывается ее длительность в минутах.
    *   Выводятся метрики регрессии: Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE).
    *   Для нескольких примеров задач выводятся: текст задачи, эталонная длительность и предсказанная длительность.
5.  **Тестирование модели приоритета**:
    *   Для каждой задачи (извлеченной из эталонных аннотаций датасета) предсказывается ее приоритет (Low, Medium, High, представленные числами 0, 1, 2).
    *   Выводится `classification_report` по качеству классификации приоритетов.
    *   Для нескольких примеров задач выводятся: текст задачи, эталонный приоритет и предсказанный приоритет.

## Ожидаемый вывод

Скрипт выведет в консоль:
*   Информацию о загрузке данных и моделей.
*   Отчеты о качестве для NER и модели приоритета.
*   Метрики ошибок для модели длительности.
*   Примеры предсказаний для каждой из моделей, чтобы можно было визуально оценить их работу на конкретных данных.

Если какие-то модели или датасет не найдены, скрипт выведет соответствующее сообщение об ошибке.

## Актуальные данные о производительности моделей (по состоянию на <ДАТА ЗАПУСКА ТЕСТА>)

Данные получены при запуске скрипта `ml/test_models/test_models.py` на полном датасете `ml/freeform_task_dataset.json`.

### 1. Модель извлечения именованных сущностей (NER)

*   **Accuracy (IOB-тегирование)**: 0.99

### 2. Модель регрессии для предсказания длительности выполнения задачи

*   **Mean Squared Error (MSE)**: 2962.36
*   **Mean Absolute Error (MAE)**: 41.47
*   **Root Mean Squared Error (RMSE)**: 54.43

### 3. Модель классификации для предсказания приоритета задачи

*   **Accuracy**: 0.79
*   **Macro Average F1-score**: 0.65
*   **Weighted Average F1-score**: 0.76
